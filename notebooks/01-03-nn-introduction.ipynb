{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f00a76e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# INTRODUCTION  TO NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Artificial neural networks (ANN) are inspired from biological neural networks and to some extend try to mimic the way the human brain is learning.  The human brain contains over 100 billion interconnected neurons and each of them one consists of three main components: soma, axon and dendrietes as can be seem in the following image (taken from https://en.wikipedia.org/wiki/Biological_neuron_model)\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"./images/soma-axon-dentrites.png\" style=\"float:center;width:420px;\"/>\n",
    "</div>\n",
    "\n",
    "### How ANN resembles biological neural networks\n",
    "\n",
    "\n",
    "The exact way of how the biological neural networks work is not fully understood; in Artificial\n",
    "Neural Networks we follow a loose analogy that tries to resemble the way the brain is functioning\n",
    "by using a linear connectivity of subsequent layeres of **Neurons** that are connected \n",
    "via **synapses** each one having its own **weight**.\n",
    "\n",
    "Based ont the analog input that each neuron receives it **fires** a new signal which is transimited\n",
    "to the following layer until it reaches the **target** value, which is just another name for the\n",
    "output that we are trying to predict.\n",
    "\n",
    "The ANN the interaction between nodes is simulated using a polynomiam which connects each neural with a previous layer of neurons as can be seen here:\n",
    "\n",
    "<img src=\"./images/ann-example.png\" style=\"width:420px;\"/>\n",
    "<br><hr>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  The simplest  Neural Network\n",
    "\n",
    "Let's start with the most simplified example of a neural and right after we will generalize \n",
    "our findings to more complicated topologies that will allow us to solve some basic problems \n",
    "that still cannot be solved following the **traditional** approach to programming.\n",
    "\n",
    "The schema of this ANN is the following:\n",
    "\n",
    "<img src=\"./images/simplest-possible-ann.png\" style=\"width:520px;\"/> \n",
    "\n",
    "In this picture, we display known values with green color,red the unknown and blue for the calculated.\n",
    "\n",
    "The unknown value is the weight ($w$) for the connection of $x$ to $y$.\n",
    "\n",
    "**It is important to understand the simple case in depth**\n",
    "\n",
    "Although this is the simplest possible NN that we are seeing here  it still can serve as the basis for very complicated solutions that conceptually are very similar to it thus it is worth it to study it in depth as it will later help us to understand way more sophisticated NN architectures.\n",
    "\n",
    "&nbsp;\n",
    "<div style=\"background:LightYellow\">\n",
    "Please do not get intimidated from the mathematical calculations that are involved here (in reallity they are very simple if you know some basic calculus) and do not hesitate to spend some time trying to repeat the mathematical proof using pen and pencil.\n",
    "</div>\n",
    "\n",
    "**The linear relationship between $x$ and $y$**\n",
    "\n",
    "If $w_0$ is the given value of the input and $y$ the expected we can express the relationship between them as follows:\n",
    "\n",
    "&nbsp;\n",
    "$\\Large y = x_0 \\times w_1$\n",
    "\n",
    "Our objective will be to find the best possible value for the **weight** than will **minimize** the Loss function which will be a measurement of how accurate our predictions are.\n",
    "\n",
    "As an example, if we set $w_1 = 2$ then the graphical representation of the above function will be as follows:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot([1, 2], [2,4]);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### The Loss (also known as Cost or Error) function\n",
    "\n",
    "The loss function of a NN compares the expected against the predicted\n",
    "values.  It  expresses how well the NN fits the training data while\n",
    "the goal of the training process is to minimize it by applying an\n",
    "iterative process which repeats the weight adjustment until a specific\n",
    "requirement will be met.\n",
    "\n",
    "In most of the times the terms Loss, Cost or Error function can be \n",
    "used as synonyms and usually they are symbolized with one of the \n",
    "L, C or E letters.\n",
    "\n",
    "\n",
    "One commonly used lossed function is **Mean Squared Error (MSE)** which is expressed by the following formula:\n",
    "\n",
    "$\\Large C = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - Y)^2$\n",
    "\n",
    "\n",
    "\n",
    "Where:\n",
    "- C: The cost (also used as loss or error).\n",
    "\n",
    "- Y: The expected (correct) value (known in advance).\n",
    "\n",
    "- $y_i$: The calculated value.\n",
    "\n",
    "As a starting point to understanding the training of the ANN it helps to make things more intuitive and think of the **Cost** function as a a unique calculation expesses by the following equation:\n",
    "\n",
    "$\\Large C = (y - Y) ^ 2$\n",
    "\n",
    "The above **Cost** function is a **parabola** that can be graphed as follows (assuming that $Y=0.4$):\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "import functools\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot(function, v1, v2, step=0.1, plot_axes=False):\n",
    "    x = [v1]\n",
    "    while x[-1] < v2:\n",
    "        x.append(x[-1]+ step)\n",
    "    y = [function(v) for v in x]\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    #  Plot axes.\n",
    "    if plot_axes:\n",
    "        plt.plot(x, [0 for _ in range(len(x))], color='gray',  linewidth=3)\n",
    "        plt.plot([0 for _ in range(len(y))], y, color='gray',  linewidth=3)\n",
    "        plt.grid( linestyle='--')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(lambda x: (x - 0.4) ** 2 , -1.4, 2.2 , step=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Find the derivative of the function\n",
    "\n",
    "If we write our function as follows:\n",
    "Using calculus I we can easily calcuate the derivate of the Loss function:\n",
    "\n",
    "$ \\Large y=(x-a)^2= x^2-2ax+a^2$\n",
    "\n",
    "so:\n",
    "\n",
    "$\\Large \\frac{dy}{dx}=2(x-a)$\n",
    "\n",
    "#### Plot the tangent lines two arbitrary points\n",
    "\n",
    "For a given point X that belongs to the loss function the equation for the tangent line to it is given by the following:\n",
    "\n",
    "$\\Large g(x) = f'(X) (x-X) + f(X)$ \n",
    " \n",
    "which graphically results to the following represenation:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mix_x, max_x = -1.4, 2.2\n",
    "a = 0.4\n",
    "plot(lambda x: (x - 0.4) ** 2 , -1.4, 2.2 , step=0.001)\n",
    "derivative = lambda x: 2 * (x-a)\n",
    "\n",
    "def tangent_line(x, X):\n",
    "     return derivative(X) * (x - X) + (X - 0.4) **2\n",
    "\n",
    "X1, X2 = 1.5, -0.5\n",
    "\n",
    "plot(functools.partial(tangent_line, X=X1) , 0.6, max_x, step=0.01)\n",
    "plot(functools.partial(tangent_line, X=X2) , -1, 0.097, step=0.01)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can notice from the above graphical represenation, the derivate of point is positive when it lies on the right of the minuum value and negative otherwise.  Based on this notification, we will try to approach the  value of x that minimizes the function by adding or subtracting a small step each time.\n",
    "\n",
    "See for example:\n",
    "\n",
    "$\\Large f'(1.5) = 2.2$\n",
    "\n",
    "$\\Large f'(-0.5) = -1.8$\n",
    "\n",
    "It is easy to prove that the minimum value of our function will be 0 and this will be the outcome of the function when x becomes a:\n",
    "\n",
    "$\\Large minf(x) = f(a) = 0$\n",
    "\n",
    "<div>\n",
    "Also we can see that the slope of the tangent line (derivative) for our function has a positive value when we are on the right of $x=a$ and negative when on the left.  This is very important for our procedeure as it dicatates the **sign** of the alteration we need to apply to x; in other words if the slope will be postive we will need to subtract a small value from the current value otherise we will add to it.\n",
    "\n",
    "So, if we pretend that we do not know the analytical solution of how to find the minimum value for the function $f(x)$ we can start from any arbitrary value of x and start moving accros the axis of x until we will find the value of x that will have the minimum value.\n",
    "\n",
    "<img src=\"./images/backprog-1.png\" style=\"width:220px;\"/>\n",
    "</div>\n",
    "\n",
    "As we can see in the above picture, we need to move the $x$ by a value $Δx$ towards the **left** direction (meaning subtracting a small value) to find a lower value for $y$.  \n",
    "\n",
    "This movement depends on two factors:\n",
    "\n",
    "**Learning rate**\n",
    "\n",
    "<div>\n",
    "<img src=\"./images/learning-rate.png\" style=\"width:620px;\"/>\n",
    "You can thing of the learning rate as the unit of weight change per each learning iteration.  \n",
    "The learning rate is one of the hyperparameters of the ANN and most of\n",
    "times finding its best value results in a trial and error procedure since\n",
    "avery small learning rate will result to very slow training process while\n",
    "a very large might not be able to find the minumum value. \n",
    "</div>\n",
    "<br>\n",
    "\n",
    "\n",
    "The following pictute helps to develop intution about how a large vs a smaller learning rate value might affect learning ([source](https://www.ibm.com/cloud/learn/gradient-descent#:~:text=Gradient%20descent%20is%20an%20optimization,each%20iteration%20of%20parameter%20updates))\n",
    "\n",
    "\n",
    "In practice the learning rate is a value that ranges anywhere from 0.01 to\n",
    "0.0001 while modern machine learning libraries provide functions that allow\n",
    "the training procedure to adjust it based on several user defined criteria.\n",
    "\n",
    "\n",
    "\n",
    "**The partial derivate of cost over weight**\n",
    "\n",
    "Expresses the impart that a specific weight has over a calculated value.\n",
    "\n",
    "It can be positive or negative and its value give us an indicator of how important the specific weight is in the calculated value.\n",
    "\n",
    "\n",
    "#### The weight adjustment\n",
    "\n",
    "During each iteration phase each weight of the ANN will be adjusted based on the following formula:\n",
    "\n",
    "$\\Large w_{n+1}= w_n - R * \\frac{\\partial C}{\\partial w}$\n",
    "\n",
    "where:\n",
    "\n",
    "| Syntax                               | Description                              |\n",
    "| -------------------------------------| -----------------------------------------|\n",
    "| wn+1                    | The weight during the n + 1 epoch            |\n",
    "|$\\Large w_n$                          | The weight during the n-th epoch         |\n",
    "|$\\Large R$                           | The learning rate                        |\n",
    "|$\\Large \\frac{\\partial C}{\\partial w}$|The partial derivative of Cost over weight|\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Adjusting the weight using back propagation\n",
    "\n",
    "In our example we only have one observation consistinting\n",
    "of one input: $x_0 = 1$ and one expected output $Y=0.5$\n",
    "\n",
    "We randomly assign a weight, let say 0.3 thus\n",
    "we know can have a prediction:\n",
    "\n",
    "$\\Large x_1 = x_0 \\times w = 1 \\times 0.3 = 0.3$\n",
    "\n",
    "since the expected output is 0.5 the **Cost** will be:\n",
    "\n",
    "$\\Large C = (x_1 - Y)^2 = (0.3 - 0.5) ^2 = 0.04$\n",
    "\n",
    "Since the only parameter that can be changed in the NN is the weight, what we need is the \n",
    "rate of the Loss over weight or more formally:\n",
    "\n",
    "$\\Large \\frac{\\partial L}{\\partial w}$\n",
    "\n",
    "\n",
    "As discussed above the derivate of the cost function will be\n",
    "\n",
    "$\\Large\\frac{\\partial C}{ \\partial x}=2(x_1 - Υ)$\n",
    "\n",
    "while the derivate of the predicted value will be:\n",
    "\n",
    "$\\Large\\frac{\\partial x}{\\partial w}=x_0$\n",
    "\n",
    "$\\Large\\Large\\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial x} \\times  \\frac{\\partial x}{\\partial w}  \\Rightarrow$\n",
    "\n",
    "\n",
    "$\\Large\\frac{\\partial L}{\\partial w} = 2(x_1 - Υ) \\times x_0 \\Rightarrow$\n",
    "\n",
    "$\\Large\\frac{\\partial L}{\\partial w} = 2(x_0 \\times w_0 - Υ) \\times x_0$\n",
    "\n",
    "\n",
    "So the new weight will be given by the following formula:\n",
    "\n",
    "$\\Large w_1 = w_0 - R\\times 2(x_0 \\times w_0 - Υ) \\times x_0$\n",
    "\n",
    "By substituding the follwing values:\n",
    "\n",
    "| Variable    | Description                     | Value|\n",
    "| ----------- | --------------------------------|-------\n",
    "| $w_0$       | The weight during the 1st epoch |0.3|\n",
    "| $R$        | The learning rate               |0.01|\n",
    "| $x_0$       | The input value for x           |1|\n",
    "| $Y$         | The expected value for Y        |0.5|\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Understanding the simpler possible ANN (consisting of only two neurons)\n",
    "\n",
    "The data that we need in order to **train** our model are the simplest possible and consider of just a pair of an input and its desiser output.  Some as simple as the following is enough for us to start our first contact with neural networks.\n",
    "\n",
    "| Input| Output |\n",
    "| --- | --- |\n",
    "| 1 | 0.5 |\n",
    "\n",
    "\n",
    "We can visualize this ANN as follows:\n",
    "\n",
    "<img src=\"./images/simplest-nn.png\" style=\"width:420px;\"/>\n",
    "\n",
    "And following the algorithm that is presented above we can code a training session as follows:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w = 0.3\n",
    "lr = 0.01\n",
    "x0 = 1\n",
    "Y = 0.5\n",
    "loss = sys.maxsize\n",
    "history = []\n",
    "\n",
    "while loss > 0.00001:\n",
    "    w = w - lr * 2 * (x0 * w - Y ) * x0\n",
    "    loss = (Y - x0*w)**2\n",
    "    history.append([loss, w])\n",
    "\n",
    "print(f\"Best estimate for w: {w:8.4}\")\n",
    "df = pd.DataFrame(history, columns = ['Loss', 'Weight'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = df.Loss.plot(title=\"Loss\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = df.Weight.plot(title=\"Weight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Adding a hidden layer to the 2 neural NN\n",
    "\n",
    "Now that we understand the simplest possible case of an ANN that consists\n",
    "of two neurons (or two layers having a single neuron each one) we can add\n",
    "more layers (again consisting of a single neuron) and see how the above\n",
    "mechanism to train the weights can be generalized.\n",
    "\n",
    "Keep the rest of the constraints, let' add one more **hidden layer** to the\n",
    "above network which now will look as follows:\n",
    "\n",
    "<img src=\"./images/ann-3-layers.png\" style=\"width:520px;\"/> \n",
    "\n",
    "We are given the values for the input x0, the weights w1 and w2 are assigned \n",
    "randomly, we are given the expected value Y and our objective is to **train**\n",
    "the ANN assigning to the weights w1 and w2 the best values we can find so the \n",
    "Cost (or loss or error) will be minimized.\n",
    "\n",
    "Repeating what we have done above, the cost function will be:\n",
    "\n",
    "$\\Large C = (x_2 - Y) ^2$ \n",
    "\n",
    "while its **gradient descent** (here derivative) will be the following:\n",
    "\n",
    "\n",
    "$\\Large\\frac{\\partial C}{\\partial x_2} = 2(x_2 - Υ)$\n",
    "\n",
    "The objective is to run serveral consecutive iterations trying to adjust the \n",
    "randomly selected weights trying to minimize the **Cost**.\n",
    "\n",
    "Assuming a learning rate of **η** the new weight for each iteration will be \n",
    "given from the following formula:\n",
    "\n",
    "$\\Large w_i' = w_i - \\eta \\times  \\frac{\\partial C}{\\partial w_i}$\n",
    "\n",
    "What we are missing for now from the above formula is the following:\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial w_i}$\n",
    "\n",
    "which can be thought as the **impact** that a particular weigtht has on\n",
    "the **Cost**.  Note though that we already have the rate of how much a change\n",
    "in the value of $\\Large x_2$ is affecting the cost $\\Large\\frac{\\partial C}{\\partial x_2}$.\n",
    "\n",
    "# PAY ATTENTION HERE !\n",
    "\n",
    "## Step 1: Calculate the next value for $\\Large w_2$\n",
    "\n",
    "This step is one of the MOST important to understand how backpropagation works, \n",
    "so please read carefully:\n",
    "\n",
    "We want to calculate: $\\Large \\frac{\\partial C}{\\partial w_2}$ so, let's try the following trick:\n",
    "\n",
    "Is it possible for us to write our expression as follows?\n",
    "\n",
    "$\\Large \\frac{\\partial ?}{\\partial w_2} \\times \\frac{\\partial C}{\\partial ?}$\n",
    "\n",
    "Think now: Can we substitue $\\Large {\\partial ?}$ with something that will result \n",
    "to two partial derivatives that is known to us? \n",
    "\n",
    "If we can do so then we would have our prolbem solved (applying the **CHAIN RULE** from Calculus)\n",
    "\n",
    "Since we already have calculated: $\\Large\\frac{\\partial C}{\\partial x_2}$\n",
    "\n",
    "we immediatly think if we can try the following:\n",
    "\n",
    "$\\Large \\frac{\\partial x_2}{\\partial w_2} \\times \\frac{\\partial C}{\\partial x_2}$\n",
    "\n",
    "or in other words to calculate the following expression:\n",
    "\n",
    "$\\Large \\frac{\\partial x_2}{\\partial w_2}$\n",
    "\n",
    "which expresses the impact of $\\Large w_2$ to $\\Large x_2$\n",
    "\n",
    "and we are in luck since this derivative is really easy to calculate:\n",
    "\n",
    "since $\\Large x_2 = x1 \\times w_2$ then \n",
    "\n",
    "$\\Large \\frac{\\partial x_2}{\\partial w_2} = x_1$\n",
    "\n",
    "\n",
    "So we can now calculate:\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial w_2} = \\Large \\frac{\\partial x_2}{\\partial w_2} \\times \\frac{\\partial C}{\\partial x_2}  = x_1 \\times 2(x_2 - Υ)$\n",
    "\n",
    "while the new value for $w_2$ for the next iteration will be:\n",
    "\n",
    "$\\Large w_2' = w_2 - \\eta \\times  x_1 \\times 2(x_2 - Υ)$\n",
    "\n",
    "## Step 2: Calculate the next value for $\\Large w_1$\n",
    "\n",
    "Now, we need to calculate: $\\Large \\frac{\\partial C}{\\partial w_1}$ \n",
    "\n",
    "we think similarly to before but with one more trick! We will have to add one more\n",
    "layer to our **chain** of derivations as follows:\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial w_1}$ \n",
    "\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial w_1} = \\Large \\frac{\\partial ? }{\\partial w_1} \\times \\Large \\frac{\\partial x_2}{\\partial ?} \\times \\frac{\\partial C}{\\partial x_2}$\n",
    "\n",
    "Now is is easly to see that the question mark should be substituted by the ${\\Large \\partial x_1}$\n",
    "\n",
    "and our expression now it will become:\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial w_1} = \\Large \\frac{\\partial x_1 }{\\partial w_1} \\times \\Large \\frac{\\partial x_2}{\\partial x_1} \\times \\frac{\\partial C}{\\partial x_2}$\n",
    "\n",
    "\n",
    "Since,\n",
    "\n",
    "$\\Large \\frac {\\partial x_1} {\\partial w_1}  = x_0$\n",
    "\n",
    "$\\Large \\frac {\\partial x_2} {\\partial x_1}  = w_2$\n",
    "\n",
    "$\\Large\\frac{\\partial C}{\\partial x_2} = 2(x_2 - Υ)$\n",
    "\n",
    "we have:\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial w_1} = x_0 \\times w_2 \\times 2(x_2 - Y)$ \n",
    "\n",
    "and \n",
    "\n",
    "$\\Large w_1' = w_1 - \\eta \\times  x_0 \\times w_2 \\times 2(x_2 - Y)$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding more hidden layers\n",
    "\n",
    "The process we have described above can now be generalized to cover as many\n",
    "hidden layers we need.\n",
    "\n",
    "Here is how we can carry on the chain rule to any number of hidden layers.\n",
    "\n",
    "Try to understand how this is possible\n",
    "\n",
    "Hint: The key is to realize that the formula:\n",
    "\n",
    "$\\Large x_i = w_i \\times x_{i-1}$\n",
    "\n",
    "allows us to calculate both:\n",
    "\n",
    "$\\Large \\frac {\\partial x_i} {\\partial w_i} = x_{i-1}$\n",
    "\n",
    "and\n",
    "\n",
    "$\\Large \\frac {\\partial x_i} {\\partial x_{i-1}} = w_i$\n",
    "\n",
    "\n",
    "\n",
    "For example lets find the new weight $w_2$\n",
    "\n",
    "<img src=\"./images/nn-multiple-hidden-layers.png\" style=\"width:620px;\"/> \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Generalizing for more than one synapsis\n",
    "\n",
    "The same process we have seen so far in the simple case of a singe connection\n",
    "between layers can easily by generalized using again the chain rule as we ca\n",
    "see here:\n",
    "\n",
    "<img src=\"./images/two-x-nn.png\" style=\"width:520px;\"/> \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "9ef37c3b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Adding a hidden layer to the 2 neural NN\n",
    "\n",
    "Now that we understand the simplest possible case of an ANN that consists\n",
    "of two neurons (or two layers having a single neuron each one) we can add\n",
    "more layers (again consisting of a single neuron) and see how the above\n",
    "mechanism to train the weights can be generalized.\n",
    "\n",
    "Keep the rest of the constraints, let' add one more **hidden layer** to the\n",
    "above network which now will look as follows:\n",
    "\n",
    "<img src=\"./images/ann-3-layers.png\" style=\"width:520px;\"/> \n",
    "\n",
    "We are given the values for the input x0, the weights w1 and w2 are assigned \n",
    "randomly, we are given the expected value Y and our objective is to **train**\n",
    "the ANN assigning to the weights w1 and w2 the best values we can find so the \n",
    "Cost (or loss or error) will be minimized.\n",
    "\n",
    "Repeating what we have done above, the cost function will be:\n",
    "\n",
    "$\\Large C = (x_2 - Y) ^2$ \n",
    "\n",
    "while its **gradient descent** (here derivative) will be the following:\n",
    "\n",
    "\n",
    "$\\Large\\frac{\\partial C}{\\partial x_2} = 2(x_2 - Υ)$\n",
    "\n",
    "The objective is to run serveral consecutive iterations trying to adjust the \n",
    "randomly selected weights trying to minimize the **Cost**.\n",
    "\n",
    "Assuming a learning rate of **η** the new weight for each iteration will be \n",
    "given from the following formula:\n",
    "\n",
    "$\\Large w_i' = w_i - \\eta \\times  \\frac{\\partial C}{\\partial w_i}$\n",
    "\n",
    "What we are missing for now from the above formula is the following:\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial w_i}$\n",
    "\n",
    "which can be thought as the **impact** that a particular weigtht has on\n",
    "the **Cost**.  Note though that we already have the rate of how much a change\n",
    "in the value of $\\Large x_2$ is affecting the cost $\\Large\\frac{\\partial C}{\\partial x_2}$.\n",
    "\n",
    "# PAY ATTENTION HERE !\n",
    "\n",
    "## Step 1: Calculate the next value for $\\Large w_2$\n",
    "\n",
    "This step is one of the MOST important to understand how backpropagation works, \n",
    "so please read carefully:\n",
    "\n",
    "We want to calculate: $\\Large \\frac{\\partial C}{\\partial w_2}$ so, let's try the following trick:\n",
    "\n",
    "Is it possible for us to write our expression as follows?\n",
    "\n",
    "$\\Large \\frac{\\partial ?}{\\partial w_2} \\times \\frac{\\partial C}{\\partial ?}$\n",
    "\n",
    "Think now: Can we substitue $\\Large {\\partial ?}$ with something that will result \n",
    "to two partial derivatives that is known to us? \n",
    "\n",
    "If we can do so then we would have our prolbem solved (applying the **CHAIN RULE** from Calculus)\n",
    "\n",
    "Since we already have calculated: $\\Large\\frac{\\partial C}{\\partial x_2}$\n",
    "\n",
    "we immediatly think if we can try the following:\n",
    "\n",
    "$\\Large \\frac{\\partial x_2}{\\partial w_2} \\times \\frac{\\partial C}{\\partial x_2}$\n",
    "\n",
    "or in other words to calculate the following expression:\n",
    "\n",
    "$\\Large \\frac{\\partial x_2}{\\partial w_2}$\n",
    "\n",
    "which expresses the impact of $\\Large w_2$ to $\\Large x_2$\n",
    "\n",
    "and we are in luck since this derivative is really easy to calculate:\n",
    "\n",
    "since $\\Large x_2 = x1 \\times w_2$ then \n",
    "\n",
    "$\\Large \\frac{\\partial x_2}{\\partial w_2} = x_1$\n",
    "\n",
    "\n",
    "So we can now calculate:\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial w_2} = \\Large \\frac{\\partial x_2}{\\partial w_2} \\times \\frac{\\partial C}{\\partial x_2}  = x_1 \\times 2(x_2 - Υ)$\n",
    "\n",
    "while the new value for $w_2$ for the next iteration will be:\n",
    "\n",
    "$\\Large w_2' = w_2 - \\eta \\times  x_1 \\times 2(x_2 - Υ)$\n",
    "\n",
    "## Step 2: Calculate the next value for $\\Large w_1$\n",
    "\n",
    "Now, we need to calculate: $\\Large \\frac{\\partial C}{\\partial w_1}$ \n",
    "\n",
    "we think similarly to before but with one more trick! We will have to add one more\n",
    "layer to our **chain** of derivations as follows:\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial w_1}$ \n",
    "\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial w_1} = \\Large \\frac{\\partial ? }{\\partial w_1} \\times \\Large \\frac{\\partial x_2}{\\partial ?} \\times \\frac{\\partial C}{\\partial x_2}$\n",
    "\n",
    "Now is is easly to see that the question mark should be substituted by the ${\\Large \\partial x_1}$\n",
    "\n",
    "and our expression now it will become:\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial w_1} = \\Large \\frac{\\partial x_1 }{\\partial w_1} \\times \\Large \\frac{\\partial x_2}{\\partial x_1} \\times \\frac{\\partial C}{\\partial x_2}$\n",
    "\n",
    "\n",
    "Since,\n",
    "\n",
    "$\\Large \\frac {\\partial x_1} {\\partial w_1}  = x_0$\n",
    "\n",
    "$\\Large \\frac {\\partial x_2} {\\partial x_1}  = w_2$\n",
    "\n",
    "$\\Large\\frac{\\partial C}{\\partial x_2} = 2(x_2 - Υ)$\n",
    "\n",
    "we have:\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial w_1} = x_0 \\times w_2 \\times 2(x_2 - Y)$ \n",
    "\n",
    "and \n",
    "\n",
    "$\\Large w_1' = w_1 - \\eta \\times  x_0 \\times w_2 \\times 2(x_2 - Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee492f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Adding more hidden layers\n",
    "\n",
    "The process we have described above can now be generalized to cover as many\n",
    "hidden layers we need.\n",
    "\n",
    "Here is how we can carry on the chain rule to any number of hidden layers.\n",
    "\n",
    "Try to understand how this is possible\n",
    "\n",
    "Hint: The key is to realize that the formula:\n",
    "\n",
    "$\\Large x_i = w_i \\times x_{i-1}$\n",
    "\n",
    "allows us to calculate both:\n",
    "\n",
    "$\\Large \\frac {\\partial x_i} {\\partial w_i} = x_{i-1}$\n",
    "\n",
    "and\n",
    "\n",
    "$\\Large \\frac {\\partial x_i} {\\partial x_{i-1}} = w_i$\n",
    "\n",
    "\n",
    "\n",
    "For example lets find the new weight $w_2$\n",
    "\n",
    "<img src=\"./images/nn-multiple-hidden-layers.png\" style=\"width:620px;\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06476b62",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#### Generalizing for more than one synapsis\n",
    "\n",
    "The same process we have seen so far in the simple case of a singe connection\n",
    "between layers can easily by generalized using again the chain rule as we ca\n",
    "see here:\n",
    "\n",
    "<img src=\"./images/two-x-nn.png\" style=\"width:520px;\"/> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9c30ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}