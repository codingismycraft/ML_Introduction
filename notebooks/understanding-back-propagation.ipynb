{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "c52abefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import functools\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3c3452",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "16949767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(function, v1, v2, step=0.1, plot_axes=False):\n",
    "    x = [v1]\n",
    "    while x[-1] < v2:\n",
    "        x.append(x[-1]+ step)\n",
    "    y = [function(v) for v in x]\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    #  Plot axes.\n",
    "    if plot_axes:\n",
    "        plt.plot(x, [0 for _ in range(len(x))], color='gray',  linewidth=3)\n",
    "        plt.plot([0 for _ in range(len(y))], y, color='gray',  linewidth=3)\n",
    "        plt.grid( linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A simplified example of Neural Network\n",
    "\n",
    "\n",
    "#### How the NN model creation differs from classical programming\n",
    "\n",
    "Note that aside from the collected data for input and output we are not given anything else. We do not have any other information about our data that would allow us to solve the problem following a **classical** programming approch.  \n",
    "\n",
    "In **classical** programming we are given a set predifined set of rules that when used they lead us to the expected result.\n",
    "\n",
    "In **machine learning** model creation we know in advance the input and the output and our objective is to find the algorithm that will allow use to predict values to new inputs that we have not seen before.\n",
    "\n",
    "In **Traditional Programming** the input(s) are passed to a deterministic flowchart (algorithm) which produces the output:\n",
    "\n",
    "![traditional-programming](./images/traditional-programming.png)\n",
    "\n",
    "When **training** a neural network we are passing both the input and the expected output to the training function which discovers and fits a set of **weights**  that resembles the relation between input and output:\n",
    "\n",
    "![training-ANN](./images/training-ANN.png)\n",
    "\n",
    "\n",
    "A **trained** ANN receives unseen input, uses the **weights** that were found during the training phase and **predicts** the corresponding output.\n",
    "\n",
    "![predicting-using-NN](./images/predicting-using-NN.png)\n",
    "\n",
    "#### The main idea behind a neural network\n",
    "\n",
    "Aartificial neural networks (ANN) are inspired from biological neural networks and to some extend try to mimic the way the human brain is learning.  The human brain contains over 100 billion interconnected neurons and each of them one consists of three main components: soma, axon and dendrietes as can be seem in the following image (taken from https://en.wikipedia.org/wiki/Biological_neuron_model)\n",
    "\n",
    "![Human Brain](./images/soma-axon-dentrites.png \"Human Brain\")\n",
    "\n",
    "#### How ANN resembles biological neural networks\n",
    "\n",
    "The exact way of how the biological neural networks work is not fully understood and the way we are porting it to the computer can be thought as a loose analogy that models the theoretical model to same extend.  A neuron receives analog input from the axon and when it \"fires\" it transmit a new analog messages over its axon.\n",
    "\n",
    "In ANN the interaction between nodes is simulated using a polynomiam which connects each neural with a previous layer of neurons as can be seen here:\n",
    "\n",
    "![ann simplified](./images/ann-example.png)\n",
    "\n",
    "\n",
    "**A simplified example of ANN**\n",
    "\n",
    "Let's start with the most simplified example of a neural network that can\n",
    "be possible and right after we will ramp it up to a more useful example\n",
    "that will allow us to solve some east problems that still cannot be solved\n",
    "following a \"standard\" approach to programming.\n",
    "\n",
    "\n",
    "> Build a model to receive a numeric value and return the half of it.\n",
    "\n",
    "We can think our model as a calculator that always divides its input by 2 and although this is over simplified case it will still be enough to intoduce us to the most basic problems we are facing when developing a neural network.\n",
    "\n",
    "We pretend that we do not now the logic that is hidden behind the input-ouput set (meaning that the output is simply the half of the input).  Still, based on what we presented above, about how the ANN work we start with the assumption that we can model the relation as the following polynomial:\n",
    "\n",
    "$\n",
    "\\Large y = x_0 \\times w_1\n",
    "$\n",
    "\n",
    "![high level view of simplest possible ANN](./images/simplest-possible-ann.png)\n",
    "\n",
    "Our objective will be to find the best possible value for the **weight** than will **minimize** the Loss function which will be a measurement of how accurate our predictions are.\n",
    "\n",
    "#### The Loss (also known as Cost or Error) function\n",
    "\n",
    "The loss function of a NN compares the expected against the predicted\n",
    "values.  It  expresses how well the NN fits the training data while\n",
    "the goal of the training process is to minimize it by applying an\n",
    "iterative process which repeats the weight adjustment until a specific\n",
    "requirement will be met.\n",
    "\n",
    "In most of the times the terms Loss, Cost or Error function can be \n",
    "used as synonyms and usually they are symbolized with one of the \n",
    "L, C or E letters.\n",
    "\n",
    "\n",
    "One commonly used lossed function is **Mean Squared Error (MSE)** which is expressed by the following formula:\n",
    "\n",
    "$\n",
    "\\Large C = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - Y)^2\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "Where:\n",
    "- C: The cost (also used as loss or error).\n",
    "\n",
    "- Y: The expected (correct) value (known in advance).\n",
    "\n",
    "- $y_i$: The calculated value.\n",
    "\n",
    "As a starting point to understanding the training of the ANN it helps to make things more intuitive and think of the **Cost** function as a a unique calculation expesses by the following equation:\n",
    "\n",
    "$\n",
    "\\Large C = (y - Y) ^ 2\n",
    "$\n",
    "\n",
    "The above **Cost** function is a **parabola** that can be graphed as follows (assuming that $Y=0.4$):\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(lambda x: (x - 0.4) ** 2 , -1.4, 2.2 , step=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Find the derivative of the function\n",
    "\n",
    "If we write our function as follows:\n",
    "Using calculus I we can easily calcuate the derivate of the Loss function:\n",
    "\n",
    "$ \n",
    "\\Large y=(x-a)^2= x^2-2ax+a^2\n",
    "$\n",
    "\n",
    "so:\n",
    "\n",
    "$\n",
    "\\Large \\frac{dy}{dx}=2(x-a)\n",
    "$\n",
    "\n",
    "#### Plot the tangent lines two arbitrary points\n",
    "\n",
    "For a given point X that belongs to the loss function the equation for the tangent line to it is given by the following:\n",
    "\n",
    " $\n",
    " \\Large g(x) = f'(X) (x-X) + f(X)\n",
    " $ \n",
    " \n",
    " which graphically results to the following represenation:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mix_x, max_x = -1.4, 2.2\n",
    "plot(function , mix_x, max_x , step=step)\n",
    "derivative = lambda x: 2 * (x-a)\n",
    "def tangent_line(x, X):\n",
    "    return derivative(X) * (x - X) + function(X)\n",
    "X1, X2 = 1.5, -0.5\n",
    "plot(functools.partial(tangent_line, X=X1) , 0.6, max_x, step=step)\n",
    "plot(functools.partial(tangent_line, X=X2) , -1, 0.097, step=step)\n",
    "\n",
    "der1 = derivative(X1)\n",
    "der2 = derivative(X2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can notice from the above graphical represenation, the derivate of point is positive when it lies on the right of the minuum value and negative otherwise.  Based on this notification, we will try to approach the  value of x that minimizes the function by adding or subtracting a small step each time.\n",
    "\n",
    "See for example:\n",
    "\n",
    "$\n",
    "\\Large f'(1.5) = 2.2\n",
    "$\n",
    "\n",
    "$\n",
    "\\Large f'(-0.5) = -1.8\n",
    "$\n",
    "\n",
    "It is easy to prove that the minimum value of our function will be 0 and this will be the outcome of the function when x becomes a:\n",
    "\n",
    "$ \n",
    "\\Large minf(x) = f(a) = 0\n",
    "$\n",
    "\n",
    "Also we can see that the slope of the tangent line (derivative) for our function has a positive value when we are on the right of $x=a$ and negative when on the left.  This is very important for our procedeure as it dicatates the **sign** of the alteration we need to apply to x; in other words if the slope will be postive we will need to subtract a small value from the current value otherise we will add to it.\n",
    "\n",
    "So, if we pretend that we do not know the analytical solution of how to find the minimum value for the function $f(x)$ we can start from any arbitrary value of x and start moving accros the axis of x until we will find the value of x that will have the minimum value.\n",
    "\n",
    "![Finding the minimum Value](./images/backprog-1.png \"Finding the minimum Value\")\n",
    "\n",
    "As we can see in the above picture, we need to move the $x$ by a value $Δx$ towards the **left** direction (meaning subtracting a small value) to find a lower value for $y$.  \n",
    "\n",
    "This movement depends on two factors:\n",
    "\n",
    "**Learning rate**\n",
    "\n",
    "You can thing of the learning rate as the unit of weight change per each learning iteration.  \n",
    "\n",
    "The learning rate is one of the hyperparameters of the ANN and most of\n",
    "times finding its best value results in a trial and error procedure since\n",
    "avery small learning rate will result to very slow training process while\n",
    "a very large might not be able to find the minumum value. \n",
    "\n",
    "The following pictute helps to develop intution about how a large vs a smaller learning rate value might affect learning ([source](./images/https://www.ibm.com/cloud/learn/gradient-descent#:~:text=Gradient%20descent%20is%20an%20optimization,each%20iteration%20of%20parameter%20updates))\n",
    "\n",
    "![Learning rate intuition](./images/learning-rate.png)\n",
    "\n",
    "\n",
    "In practice the learning rate is a value that ranges anywhere from 0.01 to\n",
    "0.0001 while modern machine learning libraries provide functions that allow\n",
    "the training procedure to adjust it based on several user defined criteria.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**The partial derivate of cost over weight**\n",
    "\n",
    "Expresses the impart that a specific weight has over a calculated value.\n",
    "\n",
    "It can be positive or negative and its value give us an indicator of how important the specific weight is in the calculated value.\n",
    "\n",
    "\n",
    "#### The weight adjustment\n",
    "\n",
    "During each iteration phase each weight of the ANN will be adjusted based on the following formula:\n",
    "\n",
    "$\n",
    "   \\Large w_{n+1}= w_n - R * \\frac{\\partial C}{\\partial w}\n",
    "$\n",
    "\n",
    "where:\n",
    "\n",
    "| Syntax                               | Description                              |\n",
    "| -------------------------------------| -----------------------------------------|\n",
    "| wn+1                    | The weight during the n + 1 epoch            |\n",
    "|$\\Large w_n$                          | The weight during the n-th epoch         |\n",
    "|$\\Large R $                           | The learning rate                        |\n",
    "|$\\Large \\frac{\\partial C}{\\partial w}$|The partial derivative of Cost over weight|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Adjusting the weight using back propagation\n",
    "\n",
    "In our example we only have one observation consistinting\n",
    "of one input: $x_0 = 1$ and one expected output $Y=0.5$\n",
    "\n",
    "We randomly assign a weight, let say 0.3 thus\n",
    "we know can have a prediction:\n",
    "\n",
    "$\\Large x_1 = x_0 \\times w = 1 \\times 0.3 = 0.3$\n",
    "\n",
    "since the expected output is 0.5 the **Cost** will be:\n",
    "\n",
    "$\\Large C = (x_1 - Y)^2 = (0.3 - 0.5) ^2 = 0.04$\n",
    "\n",
    "Since the only parameter that can be changed in the NN is the weight, what we need is the \n",
    "rate of the Loss over weight or more formally:\n",
    "\n",
    "$\\Large \\frac{\\partial L}{\\partial w}$\n",
    "\n",
    "\n",
    "As discussed above the derivate of the cost function will be\n",
    "\n",
    "$\n",
    "\\Large\\frac{\\partial C}{ \\partial x}=2(x_1 - Υ)\n",
    "$\n",
    "\n",
    "while the derivate of the predicted value will be:\n",
    "\n",
    "$\n",
    "\\Large\\frac{\\partial x}{\\partial w}=x_0\n",
    "$\n",
    "\n",
    "$\n",
    "\\Large\\Large\\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial x} \\times  \\frac{\\partial x}{\\partial w}  \\Rightarrow \n",
    "$\n",
    "\n",
    "\n",
    "$\n",
    "\\Large\\frac{\\partial L}{\\partial w} = 2(x_1 - Υ) \\times x_0 \\Rightarrow \n",
    "$\n",
    "\n",
    "$\n",
    "\\Large\\frac{\\partial L}{\\partial w} = 2(x_0 \\times w_0 - Υ) \\times x_0\n",
    "$\n",
    "\n",
    "\n",
    "So the new weight will be given by the following formula:\n",
    "\n",
    "$\n",
    "\\Large w_1 = w_0 - R\\times 2(x_0 \\times w_0 - Υ) \\times x_0\n",
    "$\n",
    "\n",
    "By substituding the follwing values:\n",
    "\n",
    "| Variable    | Description                     | Value|\n",
    "| ----------- | --------------------------------|-------\n",
    "| $w_0$       | The weight during the 1st epoch |0.3|\n",
    "| $R$        | The learning rate               |0.01|\n",
    "| $x_0$       | The input value for x           |1|\n",
    "| $Y$         | The expected value for Y        |0.5|\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Understanding the simpler possible ANN (consisting of only two neurons)\n",
    "\n",
    "The data that we need in order to **train** our model are the simplest possible and consider of just a pair of an input and its desiser output.  Some as simple as the following is enough for us to start our first contact with neural networks.\n",
    "\n",
    "| Input| Output |\n",
    "| --- | --- |\n",
    "| 1 | 0.5 |\n",
    "\n",
    "\n",
    "We can visualize this ANN as follows:\n",
    "\n",
    "![simplest-nn](./images/simplest-nn.png)\n",
    "\n",
    "And following the algorithm that is presented above we can code a training session as follows:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w = 0.3\n",
    "lr = 0.01\n",
    "x0 = 1\n",
    "Y = 0.5\n",
    "loss = sys.maxsize\n",
    "history = []\n",
    "\n",
    "while loss > 0.00001:\n",
    "    w = w - lr * 2 * (x0 * w - Y ) * x0\n",
    "    loss = (Y - x0*w)**2\n",
    "    history.append([loss, w])\n",
    "\n",
    "print(f\"Best estimate for w: {w:8.4}\")\n",
    "df = pd.DataFrame(history, columns = ['Loss', 'Weight'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = df.Loss.plot(title=\"Loss\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = df.Weight.plot(title=\"Weight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Adding a hidden layer to the 2 neural NN\n",
    "\n",
    "Now that we understand the simplest possible case of an ANN that consists\n",
    "of two neurons (or two layers having a single neuron each one) we can add\n",
    "more layers (again consisting of a single neuron) and see how the above\n",
    "mechanism to train the weights can be generalized.\n",
    "\n",
    "Keep the rest of the constraints, let' add one more **hidden layer** to the\n",
    "above network which now will look as follows:\n",
    "\n",
    "![3 layered NN](./images/ann-3-layers.png)\n",
    "\n",
    "We are given the values for the input x0, the weights w1 and w2 are assigned \n",
    "randomly, we are given the expected value Y and our objective is to **train**\n",
    "the ANN assigning to the weights w1 and w2 the best values we can find so the \n",
    "Cost (or loss or error) will be minimized.\n",
    "\n",
    "Repeating what we have done above, the cost function will be:\n",
    "\n",
    "$\\Large C = (x_2 - Y) ^2$ \n",
    "\n",
    "while its **gradient descent** (here derivative) will be the following:\n",
    "\n",
    "\n",
    "$\n",
    "\\Large\\frac{\\partial C}{\\partial x_2} = 2(x_2 - Υ) \n",
    "$\n",
    "\n",
    "The objective is to run serveral consecutive iterations trying to adjust the \n",
    "randomly selected weights trying to minimize the **Cost**.\n",
    "\n",
    "Assuming a learning rate of **η** the new weight for each iteration will be \n",
    "given from the following formula:\n",
    "\n",
    "$\\Large w_i' = w_i - \\eta \\times  \\frac{\\partial C}{\\partial w_i}$\n",
    "\n",
    "What we are missing for now from the above formula is the following:\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial w_i}$\n",
    "\n",
    "which can be thought as the **impact** that a particular weigtht has on\n",
    "the **Cost**.  Note though that we already have the rate of how much a change\n",
    "in the value of $\\Large x_2$ is affecting the cost $\\Large\\frac{\\partial C}{\\partial x_2}$.\n",
    "\n",
    "# PAY ATTENTION HERE !\n",
    "\n",
    "## Step 1: Calculate the next value for $\\Large w_2$\n",
    "\n",
    "This step is one of the MOST important to understand how backpropagation works, \n",
    "so please read carefully:\n",
    "\n",
    "We want to calculate: $\\Large \\frac{\\partial C}{\\partial w_2}$ so, let's try the following trick:\n",
    "\n",
    "Is it possible for us to write our expression as follows?\n",
    "\n",
    "$\\Large \\frac{\\partial ?}{\\partial w_2} \\times \\frac{\\partial C}{\\partial ?}$\n",
    "\n",
    "Think now: Can we substitue $\\Large {\\partial ?}$ with something that will result \n",
    "to two partial derivatives that is known to us? \n",
    "\n",
    "If we can do so then we would have our prolbem solved (applying the **CHAIN RULE** from Calculus)\n",
    "\n",
    "Since we already have calculated: $\\Large\\frac{\\partial C}{\\partial x_2}$\n",
    "\n",
    "we immediatly think if we can try the following:\n",
    "\n",
    "$\\Large \\frac{\\partial x_2}{\\partial w_2} \\times \\frac{\\partial C}{\\partial x_2}$\n",
    "\n",
    "or in other words to calculate the following expression:\n",
    "\n",
    "$\\Large \\frac{\\partial x_2}{\\partial w_2}$\n",
    "\n",
    "which expresses the impact of $\\Large w_2$ to $\\Large x_2$\n",
    "\n",
    "and we are in luck since this derivative is really easy to calculate:\n",
    "\n",
    "since $\\Large x_2 = x1 \\times w_2$ then \n",
    "\n",
    "$\n",
    "\\Large \\frac{\\partial x_2}{\\partial w_2} = x_1 \n",
    "$\n",
    "\n",
    "\n",
    "So we can now calculate:\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial w_2} = \\Large \\frac{\\partial x_2}{\\partial w_2} \\times \\frac{\\partial C}{\\partial x_2}  = x_1 \\times 2(x_2 - Υ)  $\n",
    "\n",
    "while the new value for $w_2$ for the next iteration will be:\n",
    "\n",
    "$\\Large w_2' = w_2 - \\eta \\times  x_1 \\times 2(x_2 - Υ)$\n",
    "\n",
    "## Step 2: Calculate the next value for $\\Large w_1$\n",
    "\n",
    "Now, we need to calculate: $\\Large \\frac{\\partial C}{\\partial w_1}$ \n",
    "\n",
    "we think similarly to before but with one more trick! We will have to add one more\n",
    "layer to our **chain** of derivations as follows:\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial w_1}$ \n",
    "\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial w_1} = \\Large \\frac{\\partial ? }{\\partial w_1} \\times \\Large \\frac{\\partial x_2}{\\partial ?} \\times \\frac{\\partial C}{\\partial x_2} $\n",
    "\n",
    "Now is is easly to see that the question mark should be substituted by the ${\\Large \\partial x_1}$\n",
    "\n",
    "and our expression now it will become:\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial w_1} = \\Large \\frac{\\partial x_1 }{\\partial w_1} \\times \\Large \\frac{\\partial x_2}{\\partial x_1} \\times \\frac{\\partial C}{\\partial x_2} $\n",
    "\n",
    "\n",
    "Since,\n",
    "\n",
    "$\\Large \\frac {\\partial x_1} {\\partial w_1}  = x_0$\n",
    "\n",
    "$\\Large \\frac {\\partial x_2} {\\partial x_1}  = w_2$\n",
    "\n",
    "$\\Large\\frac{\\partial C}{\\partial x_2} = 2(x_2 - Υ)$\n",
    "\n",
    "we have:\n",
    "\n",
    "$\\Large \\frac{\\partial C}{\\partial w_1} = x_0 \\times w_2 \\times 2(x_2 - Y) $ \n",
    "\n",
    "and \n",
    "\n",
    "$\\Large w_1' = w_1 - \\eta \\times  x_0 \\times w_2 \\times 2(x_2 - Y)$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}