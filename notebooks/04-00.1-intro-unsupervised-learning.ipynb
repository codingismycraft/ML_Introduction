{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b301df8",
   "metadata": {},
   "source": [
    "# Supervised and unsupervised ML problems\n",
    "\n",
    "ML problems can be categorized in two main groups based on whether\n",
    "\"target\" values (also known as labels) are available or not.\n",
    "\n",
    "Supervised are problems where we have both the input and the desired output\n",
    "available and we want to discover a model that will allow us to predict new\n",
    "values from unseen data.\n",
    "\n",
    "Unsupervised learning problems consist a category where we are only given a\n",
    "set of data without having any target that we are trying to match.\n",
    "\n",
    "The commonly used unsupervised algorithms follow to two main categories:\n",
    "**Clusterization** and **Dimensionality reduction**.\n",
    "\n",
    "### Clusterization\n",
    "\n",
    "Clustering is the process of grouping the data based on their features.\n",
    "\n",
    "The most commonly used methods for clusterization are the following:\n",
    "\n",
    "- k-means\n",
    "- Gaussian mixture models\n",
    "- Hierarchical clustering\n",
    "\n",
    "### Dimensionality reduction\n",
    "\n",
    "This problem occurs when we have many features and need to reduce their\n",
    "number while not loosing a lot of the contained information.\n",
    "\n",
    "The most common methods for dimensionality reduction are the following:\n",
    "\n",
    "- Principal Component Analysis (PCA)\n",
    "- Multidimensional Scaling\n",
    "- Manifold Learning\n",
    "\n",
    "The most popular algorithms from these two categories that we will examine\n",
    "in detail are: **k-means** and **Principal Component Analysis**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The K-means algorithm\n",
    "\n",
    "## Input\n",
    "\n",
    "We are given a collection of **samples** (in their simpler form they are 2-D points but the algorithm can be generalized any number of dimentions). Each **sample** can be expresesed as a tuple **(p1, p2, ... pn)**.\n",
    "\n",
    "### Input Example\n",
    "\n",
    "The following list of 2D points can be used as the input to k-means:\n",
    "\n",
    "```json\n",
    "[\n",
    "    (-6.73, 6.5), (-4.43, 7.24), (-4.17, 6.22), (-3.39, 7.73), (-2.93, 6.53), \n",
    "    (5.25, -7.29), (6.76, -8.36), (6.95, -6.75), (7.06, -7.12), (7.4, -6.86)\n",
    "]\n",
    "```\n",
    "\n",
    "A graphical representation of this input can be seen here:\n",
    "\n",
    "\n",
    "<img src=\"./images/k-means-input-example.png\" style=\"width:320px\"/>\n",
    "\n",
    "\n",
    "\n",
    "## Output\n",
    "\n",
    "The output consists of the following:\n",
    "\n",
    "- The centers of each cluster (list of points) (centroids)\n",
    "\n",
    "- The corresponding centroid for each point.\n",
    "\n",
    "### Output Example\n",
    "\n",
    "Following the same data we have seen about as an example, the output of the \n",
    "k-means algorithm will consist of the following:\n",
    "\n",
    "**The coordinates of the centroids**\n",
    "```json\n",
    "[\n",
    "    [ 6.68, -7.27], \n",
    "    [-4.33,  6.84]\n",
    "]\n",
    "```\n",
    "and the list of corresponding centroid for each data point:\n",
    "\n",
    "```json\n",
    "[0, 0, 0, 1, 1, 1, 1, 0, 1, 0]\n",
    "```\n",
    "\n",
    "(where 0 and 1 represent the cetroid)\n",
    "\n",
    "A graphical representation of this output can be seen here:\n",
    "\n",
    "<img src=\"./images/k-means-output-example.png\" style=\"width:320px\"/>\n",
    "\n",
    "Note that the green dots represent the centroids while the colored data points the corresponding clusters.\n",
    "\n",
    "\n",
    "## Implementation\n",
    "\n",
    "- Specify an arbitrary number of cluster that you will be looking for.\n",
    "- Select N random points (N is the number of clusters) called centroids.\n",
    "- Iterate through all the available points and find the closest centroid.\n",
    "- Move each centroid to the average point for the assigned points\n",
    "- Repeat until there are no changes any more.\n",
    "\n",
    "## Pros\n",
    "\n",
    "- The k-means algorithm is very simple to implement\n",
    "- It can generalize to clusters of different shapes \n",
    "\n",
    "## Cons\n",
    "\n",
    "- We must pick the number of clusters \"by hand\"\n",
    "- Having clusters of significantly different size might generate wrong results\n",
    "- Sensitive to outliers (we must remove them before training the model)\n",
    "\n",
    "## Performance\n",
    "\n",
    "The k-means algorithm can become very slow (NP) as we are increasing the number \n",
    "of points and clusters. \n",
    "\n",
    "One interesting paper to reduce the complexity of k-means:\n",
    "\n",
    "https://ieeexplore.ieee.org/document/7065640\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sample data that can be used for clustering"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "\n",
    "# Create random features to use for testing.\n",
    "features, _ = make_blobs(\n",
    "    n_samples = 400,\n",
    "    n_features = 2, \n",
    "    centers = n_clusters,\n",
    "    cluster_std = 5.8,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "plt.scatter(features[:,0], features[:,1])\n",
    "plt.title(\"Raw data points.\")\n",
    "plt.show()\n",
    "\n",
    "features = pd.DataFrame(features, columns=[\"x\", \"y\"])\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "kmeans.fit(features)\n",
    "sns.scatterplot(x = features[\"x\"], y = features[\"y\"], hue=kmeans.labels_)\n",
    "\n",
    "kmeans.cluster_centers_\n",
    "centroids = pd.DataFrame(kmeans.cluster_centers_, columns=[\"x\", \"y\"])\n",
    "_ = plt.scatter(centroids[\"x\"],centroids[\"y\"],color='red', marker=\"o\", s=213)\n",
    "plt.title(\"Clustered points.\")\n",
    "_ = plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Elbow Method\n",
    "\n",
    "As we have seen in the above example when training a k-means model we need\n",
    "to provide the desired number of clusters as a **hyperparameter** thus we \n",
    "have to rely on intuition or trial an error to discover the centroids.\n",
    "\n",
    "An alternative way that can be used to discover the **optimal** number of\n",
    "clusters is to use the **Elbow Method**\n",
    "\n",
    "Before we continue with the **Elbow Method** we need to define the following\n",
    "new metrics that we use:\n",
    "\n",
    "- Distortion\n",
    "\n",
    "   The average of the squared distances from the centroids for all the data\n",
    "   points.\n",
    "\n",
    "- Inertia\n",
    "\n",
    "   The sum of the squared distances from the centroids.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_clustered_data(n_clusters, n_samples, cluster_std=3):\n",
    "    features, _ = make_blobs(\n",
    "        n_samples = n_samples,\n",
    "        n_features = 2, \n",
    "        centers = n_clusters,\n",
    "        cluster_std = 5.8,\n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame(features, columns=[\"x\", \"y\"])\n",
    "\n",
    "df = make_clustered_data(5, 400)\n",
    "plt.scatter(df[\"x\"], df[\"y\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for n_clusters in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(df)\n",
    "    print(n_clusters, kmeans.inertia_)\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9507ab24",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 42893.311772733614\n",
      "2 24147.852767842498\n",
      "3 17760.406637580087\n",
      "4 13472.201988808696\n",
      "5 10830.087418644594\n",
      "6 9148.7718482045\n",
      "7 7952.023411151734\n",
      "8 6971.388675630955\n",
      "9 6241.384809827611\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for n_clusters in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(df)\n",
    "    print(n_clusters, kmeans.inertia_)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5224d754",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for n_clusters in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(df)\n",
    "    print(n_clusters, kmeans.inertia_)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "eea75155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 43921.24833095626\n",
      "2 25073.776508754338\n",
      "3 17440.15006189052\n",
      "4 14267.564003064444\n",
      "5 11729.03466687978\n",
      "6 9976.431747715862\n",
      "7 8577.02274646873\n",
      "8 7566.456152102596\n",
      "9 6638.317941786655\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for n_clusters in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(df)\n",
    "    print(n_clusters, kmeans.inertia_)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}